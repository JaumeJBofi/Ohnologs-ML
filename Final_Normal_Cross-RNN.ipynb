{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_metrics as km\n",
    "from keras import initializers\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import regularizers\n",
    "from custom_callbacks import LrFinder\n",
    "from custom_callbacks import CycleLearner\n",
    "from custom_callbacks import reset_weights\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from time import time\n",
    "from keras.layers import Input, Dense,Dropout,BatchNormalization,LSTM,GRU,Bidirectional,Conv2D, MaxPool2D, Flatten, GlobalAvgPool2D, GlobalMaxPool2D,merge,CuDNNGRU,CuDNNLSTM\n",
    "from keras.models import Model,Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import math\n",
    "import json\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "\n",
    "# Get the dictionary containing each metric and the loss for each epoch\n",
    "\n",
    "train_filepath = \"train_workspace/\"\n",
    "animalList = [\"Human\",\"Pig\",\"Chicken\",\"Rat\",\"Mouse\",\"Dog\"]\n",
    "levelList = [\"Strict\",\"Relaxed\",\"Intermediate\"]\n",
    "dataList = [\"Ohnologs\",\"No-Ohnologs\",\"Paralogs\"]\n",
    "dataNameList = [\"ohnologs\",\"no-ohnologs\",\"paralog\"]\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_level = \"Strict\"\n",
    "kmer_chosen = 8\n",
    "type_chosen = \"cdna\"\n",
    "\n",
    "dataset_complete = pd.read_pickle(train_filepath + working_level + \"/datasets/2_no_seq_paralog_dataset_complete-\" + str(kmer_chosen) + \"-\" + type_chosen + \".pkl\")\n",
    "dataset_complete_soft = pd.read_pickle(train_filepath + working_level + \"/datasets/3_no_seq_paralog_dataset_complete-\" + str(kmer_chosen) + \"-\" + type_chosen + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Specie_Chicken</th>\n",
       "      <th>Specie_Dog</th>\n",
       "      <th>Specie_Human</th>\n",
       "      <th>Specie_Mouse</th>\n",
       "      <th>Specie_Pig</th>\n",
       "      <th>Specie_Rat</th>\n",
       "      <th>Is_Ohnolog</th>\n",
       "      <th>Is_Paralog</th>\n",
       "      <th>Sequence-1 GC</th>\n",
       "      <th>Sequence-2 Length</th>\n",
       "      <th>...</th>\n",
       "      <th>Aligment Length_Total</th>\n",
       "      <th>Nr Mismatch_Total</th>\n",
       "      <th>Nr Gap Open_Total</th>\n",
       "      <th>Evalue_Total</th>\n",
       "      <th>Bit Score_Total</th>\n",
       "      <th>Nr Hits</th>\n",
       "      <th>Seq-2-Biotype_protein_coding</th>\n",
       "      <th>Sequence 1 Numerical</th>\n",
       "      <th>Sequence 2 Numerical</th>\n",
       "      <th>Dup_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429090</td>\n",
       "      <td>0.164613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046193</td>\n",
       "      <td>0.056091</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.457654</td>\n",
       "      <td>0.012462</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5,1.5,1.5,1.5,0.5,0.5,-1.5,-1.5,0.5,1.5,0.5,...</td>\n",
       "      <td>-1.5,0.5,1.5,-1.5,1.5,0.5,1.5,1.5,1.5,1.5,0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.441268</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.011272</td>\n",
       "      <td>0.613371</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5,0.5,1.5,1.5,1.5,1.5,0.5,0.5,0.5,-1.5,-1.5,...</td>\n",
       "      <td>-1.5,1.5,-1.5,0.5,-1.5,-1.5,1.5,-1.5,0.5,1.5,0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357972</td>\n",
       "      <td>0.068582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049161</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.171352</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.5,0.5,1.5,0.5,-1.5,-1.5,0.5,0.5,1.5,0.5,1.5...</td>\n",
       "      <td>-1.5,0.5,-1.5,1.5,-1.5,-1.5,1.5,0.5,1.5,-1.5,-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684226</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011033</td>\n",
       "      <td>0.008087</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.464210</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,-1.5,0.5,0...</td>\n",
       "      <td>0.5,-1.5,-1.5,0.5,-1.5,-1.5,0.5,0.5,0.5,-1.5,0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402629</td>\n",
       "      <td>0.104322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016728</td>\n",
       "      <td>0.012836</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.140627</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5,0.5,0.5,1.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0....</td>\n",
       "      <td>-1.5,0.5,0.5,0.5,-1.5,1.5,1.5,0.5,1.5,0.5,0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Specie_Chicken  Specie_Dog  Specie_Human  Specie_Mouse  Specie_Pig  \\\n",
       "0               0           0             1             0           0   \n",
       "1               0           0             1             0           0   \n",
       "2               0           0             1             0           0   \n",
       "3               0           0             1             0           0   \n",
       "4               0           0             1             0           0   \n",
       "\n",
       "   Specie_Rat  Is_Ohnolog  Is_Paralog  Sequence-1 GC  Sequence-2 Length  ...  \\\n",
       "0           0           1         0.0       0.429090           0.164613  ...   \n",
       "1           0           1         0.0       0.441268           0.142236  ...   \n",
       "2           0           1         0.0       0.357972           0.068582  ...   \n",
       "3           0           1         0.0       0.684226           0.161686  ...   \n",
       "4           0           1         0.0       0.402629           0.104322  ...   \n",
       "\n",
       "   Aligment Length_Total  Nr Mismatch_Total  Nr Gap Open_Total  Evalue_Total  \\\n",
       "0               0.046193           0.056091           0.058824      0.457654   \n",
       "1               0.015315           0.012044           0.011272      0.613371   \n",
       "2               0.049161           0.055012           0.027174      0.171352   \n",
       "3               0.011033           0.008087           0.007246      0.464210   \n",
       "4               0.016728           0.012836           0.021739      0.140627   \n",
       "\n",
       "   Bit Score_Total   Nr Hits  Seq-2-Biotype_protein_coding  \\\n",
       "0         0.012462  0.008333                             0   \n",
       "1         0.008681  0.013235                             1   \n",
       "2         0.018926  0.003922                             1   \n",
       "3         0.006696  0.019118                             0   \n",
       "4         0.008341  0.003922                             0   \n",
       "\n",
       "                                Sequence 1 Numerical  \\\n",
       "0  0.5,1.5,1.5,1.5,0.5,0.5,-1.5,-1.5,0.5,1.5,0.5,...   \n",
       "1  0.5,0.5,1.5,1.5,1.5,1.5,0.5,0.5,0.5,-1.5,-1.5,...   \n",
       "2  -1.5,0.5,1.5,0.5,-1.5,-1.5,0.5,0.5,1.5,0.5,1.5...   \n",
       "3  0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,-1.5,0.5,0...   \n",
       "4  0.5,0.5,0.5,1.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0....   \n",
       "\n",
       "                                Sequence 2 Numerical  Dup_Class  \n",
       "0  -1.5,0.5,1.5,-1.5,1.5,0.5,1.5,1.5,1.5,1.5,0.5,...          1  \n",
       "1  -1.5,1.5,-1.5,0.5,-1.5,-1.5,1.5,-1.5,0.5,1.5,0...          1  \n",
       "2  -1.5,0.5,-1.5,1.5,-1.5,-1.5,1.5,0.5,1.5,-1.5,-...          1  \n",
       "3  0.5,-1.5,-1.5,0.5,-1.5,-1.5,0.5,0.5,0.5,-1.5,0...          1  \n",
       "4  -1.5,0.5,0.5,0.5,-1.5,1.5,1.5,0.5,1.5,0.5,0.5,...          1  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classes = []\n",
    "for index, row in dataset_complete_soft.iterrows():\n",
    "    if(row[\"Is_Ohnolog\"] == 1):\n",
    "        new_classes.append(1)\n",
    "    else:\n",
    "        if(row[\"Is_Paralog\"] == 0):\n",
    "            new_classes.append(0)\n",
    "        else:\n",
    "            new_classes.append(2)\n",
    "dataset_complete_soft[\"Dup_Class\"] = new_classes\n",
    "dataset_complete_soft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_complete_soft[\"Sequence 1 Numerical\"].values[0].split(\",\")[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\"A\",\"B\",\"C\"]\n",
    "a[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_embeddings(df):\n",
    "    sequence_1_array = np.ndarray(shape = (len(df),), dtype=object)\n",
    "    sequence_2_array = np.ndarray(shape = (len(df),), dtype=object)\n",
    "    i = 0\n",
    "    for index,row in df.iterrows():\n",
    "        numerical_1_list = row[\"Sequence 1 Numerical\"].split(\",\")[:-1]\n",
    "        numerical_1_list = list(map(float, numerical_1_list))\n",
    "    \n",
    "        numerical_2_list = row[\"Sequence 2 Numerical\"].split(\",\")[:-1]\n",
    "        numerical_2_list = list(map(float, numerical_2_list))\n",
    "        \n",
    "        sequence_1_array[i] = np.array(numerical_1_list).reshape(len(numerical_1_list),1)\n",
    "        sequence_2_array[i] = np.array(numerical_2_list).reshape(len(numerical_2_list),1)\n",
    "        i += 1\n",
    "    \n",
    "    return (sequence_1_array,sequence_2_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Specie_Chicken</th>\n",
       "      <th>Specie_Dog</th>\n",
       "      <th>Specie_Human</th>\n",
       "      <th>Specie_Mouse</th>\n",
       "      <th>Specie_Pig</th>\n",
       "      <th>Specie_Rat</th>\n",
       "      <th>Is_Ohnolog</th>\n",
       "      <th>Is_Paralog</th>\n",
       "      <th>Sequence-1 GC</th>\n",
       "      <th>Sequence-2 Length</th>\n",
       "      <th>...</th>\n",
       "      <th>Aligment Length_Total</th>\n",
       "      <th>Nr Mismatch_Total</th>\n",
       "      <th>Nr Gap Open_Total</th>\n",
       "      <th>Evalue_Total</th>\n",
       "      <th>Bit Score_Total</th>\n",
       "      <th>Nr Hits</th>\n",
       "      <th>Seq-2-Biotype_protein_coding</th>\n",
       "      <th>Sequence 1 Numerical</th>\n",
       "      <th>Sequence 2 Numerical</th>\n",
       "      <th>Dup_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429090</td>\n",
       "      <td>0.163814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046193</td>\n",
       "      <td>0.056091</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.457654</td>\n",
       "      <td>0.012462</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5,1.5,1.5,1.5,0.5,0.5,-1.5,-1.5,0.5,1.5,0.5,...</td>\n",
       "      <td>-1.5,0.5,1.5,-1.5,1.5,0.5,1.5,1.5,1.5,1.5,0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.441268</td>\n",
       "      <td>0.141415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>0.011272</td>\n",
       "      <td>0.613371</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5,0.5,1.5,1.5,1.5,1.5,0.5,0.5,0.5,-1.5,-1.5,...</td>\n",
       "      <td>-1.5,1.5,-1.5,0.5,-1.5,-1.5,1.5,-1.5,0.5,1.5,0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357972</td>\n",
       "      <td>0.067690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049161</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.171352</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.5,0.5,1.5,0.5,-1.5,-1.5,0.5,0.5,1.5,0.5,1.5...</td>\n",
       "      <td>-1.5,0.5,-1.5,1.5,-1.5,-1.5,1.5,0.5,1.5,-1.5,-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684226</td>\n",
       "      <td>0.160883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011033</td>\n",
       "      <td>0.008087</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.464210</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.019118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,-1.5,0.5,0...</td>\n",
       "      <td>0.5,-1.5,-1.5,0.5,-1.5,-1.5,0.5,0.5,0.5,-1.5,0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402629</td>\n",
       "      <td>0.103464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016728</td>\n",
       "      <td>0.012836</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.140627</td>\n",
       "      <td>0.008341</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5,0.5,0.5,1.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0....</td>\n",
       "      <td>-1.5,0.5,0.5,0.5,-1.5,1.5,1.5,0.5,1.5,0.5,0.5,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Specie_Chicken  Specie_Dog  Specie_Human  Specie_Mouse  Specie_Pig  \\\n",
       "0               0           0             1             0           0   \n",
       "1               0           0             1             0           0   \n",
       "2               0           0             1             0           0   \n",
       "3               0           0             1             0           0   \n",
       "4               0           0             1             0           0   \n",
       "\n",
       "   Specie_Rat  Is_Ohnolog  Is_Paralog  Sequence-1 GC  Sequence-2 Length  ...  \\\n",
       "0           0           1         0.0       0.429090           0.163814  ...   \n",
       "1           0           1         0.0       0.441268           0.141415  ...   \n",
       "2           0           1         0.0       0.357972           0.067690  ...   \n",
       "3           0           1         0.0       0.684226           0.160883  ...   \n",
       "4           0           1         0.0       0.402629           0.103464  ...   \n",
       "\n",
       "   Aligment Length_Total  Nr Mismatch_Total  Nr Gap Open_Total  Evalue_Total  \\\n",
       "0               0.046193           0.056091           0.058824      0.457654   \n",
       "1               0.015315           0.012044           0.011272      0.613371   \n",
       "2               0.049161           0.055012           0.027174      0.171352   \n",
       "3               0.011033           0.008087           0.007246      0.464210   \n",
       "4               0.016728           0.012836           0.021739      0.140627   \n",
       "\n",
       "   Bit Score_Total   Nr Hits  Seq-2-Biotype_protein_coding  \\\n",
       "0         0.012462  0.008333                             0   \n",
       "1         0.008681  0.013235                             1   \n",
       "2         0.018926  0.003922                             1   \n",
       "3         0.006696  0.019118                             0   \n",
       "4         0.008341  0.003922                             0   \n",
       "\n",
       "                                Sequence 1 Numerical  \\\n",
       "0  0.5,1.5,1.5,1.5,0.5,0.5,-1.5,-1.5,0.5,1.5,0.5,...   \n",
       "1  0.5,0.5,1.5,1.5,1.5,1.5,0.5,0.5,0.5,-1.5,-1.5,...   \n",
       "2  -1.5,0.5,1.5,0.5,-1.5,-1.5,0.5,0.5,1.5,0.5,1.5...   \n",
       "3  0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,-1.5,0.5,0...   \n",
       "4  0.5,0.5,0.5,1.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0....   \n",
       "\n",
       "                                Sequence 2 Numerical  Dup_Class  \n",
       "0  -1.5,0.5,1.5,-1.5,1.5,0.5,1.5,1.5,1.5,1.5,0.5,...          1  \n",
       "1  -1.5,1.5,-1.5,0.5,-1.5,-1.5,1.5,-1.5,0.5,1.5,0...          1  \n",
       "2  -1.5,0.5,-1.5,1.5,-1.5,-1.5,1.5,0.5,1.5,-1.5,-...          1  \n",
       "3  0.5,-1.5,-1.5,0.5,-1.5,-1.5,0.5,0.5,0.5,-1.5,0...          1  \n",
       "4  -1.5,0.5,0.5,0.5,-1.5,1.5,1.5,0.5,1.5,0.5,0.5,...          1  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classes = []\n",
    "for index, row in dataset_complete.iterrows():\n",
    "    if(row[\"Is_Ohnolog\"] == 1):\n",
    "        new_classes.append(1)\n",
    "    else:\n",
    "        if(row[\"Is_Paralog\"] == 0):\n",
    "            new_classes.append(0)\n",
    "        else:\n",
    "            new_classes.append(2)\n",
    "dataset_complete[\"Dup_Class\"] = new_classes\n",
    "dataset_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_columns = [\"Percent Identical Matches\",\"Aligment Length\",\"Nr Mismatch\",\"Nr Gap Open\",\"Evalue\",\"Bit Score\"]\n",
    "\n",
    "e_values_columns = [\"Evalue_Total\",\"Evalue_High\",\"Evalue_Medium\",\"Evalue_Low\"]\n",
    "\n",
    "blast_types = [\"High\",\"Medium\",\"Low\",\"Total\"]\n",
    "blast_col_Total = [i + \"_Total\" for i in blast_columns] \n",
    "blast_col_High = [i + \"_High\" for i in blast_columns]\n",
    "blast_col_Medium = [i + \"_Medium\" for i in blast_columns]\n",
    "blast_col_Low = [i + \"_Low\" for i in blast_columns]\n",
    "\n",
    "blast_cols_levels = blast_col_High + blast_col_Medium + blast_col_Low\n",
    "total_columns = (blast_col_High + blast_col_Medium + blast_col_Low + blast_col_Total + [\"Nr Hits\"])\n",
    "\n",
    "non_training_meta_features = [\"Sequence-1\",\"Sequence-2\",\"Sequence-1 Id\",\"Sequence-1-Transcript Id\",\"Sequence-2 Id\",\"Sequence-2-Transcript Id\",\"Sequence-1-Transcript-Version\",\"Sequence-2-Transcript-Version\"]\n",
    "\n",
    "sequence_1_metadata = [\"Sequence-1 GC\",\"Sequence-1 Length\",\"Sequence-1-Chromosome\",\"Seq-1-Biotype_protein_coding\"]\n",
    "sequence_2_metadata = [\"Sequence-2 GC\",\"Sequence-2 Length\",\"Sequence-2-Chromosome\",\"Seq-2-Biotype_protein_coding\"]\n",
    "\n",
    "sequence_1_metadata_diferential = [\"Sequence-1 GC\",\"Sequence-1 Length\"]\n",
    "sequence_2_metadata_diferential = [\"Sequence-2 GC\",\"Sequence-2 Length\"]\n",
    "\n",
    "sequence_1_metadata_categorical = [\"Sequence-1-Chromosome\",\"Seq-1-Biotype_protein_coding\"]\n",
    "sequence_2_metadata_categorical = [\"Sequence-2-Chromosome\",\"Seq-2-Biotype_protein_coding\"]\n",
    "\n",
    "species_metadata = [\"Specie_Chicken\",\"Specie_Dog\",\"Specie_Human\",\"Specie_Mouse\",\"Specie_Pig\",\"Specie_Rat\"]\n",
    "\n",
    "emb_size = 100\n",
    "if(type_chosen == \"cdna2\"):\n",
    "    emb_size = 200\n",
    "embedding_1_cols = [\"Embedding1_\" + str(i) for i in range(0,emb_size)]\n",
    "embedding_2_cols = [\"Embedding2_\" + str(i) for i in range(0,emb_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test(df):\n",
    "    df.sample(frac=1,random_state=7)\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2,random_state=9,stratify=df[\"Is_Ohnolog\"])\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2,random_state=3,stratify=df_train[\"Is_Ohnolog\"])\n",
    "    return (df_train,df_val,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_soft(df):\n",
    "    df.sample(frac=1,random_state=7)\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2,random_state=9,stratify=df[\"Dup_Class\"])\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.2,random_state=3,stratify=df_train[\"Dup_Class\"])\n",
    "    return (df_train,df_val,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_log(log,name):\n",
    "    log_dict = log.history    \n",
    "    json.dump(log_dict, open(train_filepath + working_level + \"/model_run_history/\" + name + \".json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log(name):\n",
    "     return json.load(open(train_filepath + working_level + \"/model_run_history/\" + name + \".json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(keras.callbacks.Callback):\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.metric_cache = {}        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.step += 1\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log_default = ''            \n",
    "            metrics_log_main = ''            \n",
    "            metrics_log_aux = ''            \n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display             \n",
    "                if(\"main\" in k):\n",
    "                    if abs(val) > 1e-3:\n",
    "                        metrics_log_main += ' - %s: %.4f' % (k, val)\n",
    "                        continue      \n",
    "                    else:\n",
    "                        metrics_log_main += ' - %s: %.4e' % (k, val)\n",
    "                        continue                    \n",
    "                        \n",
    "                if(\"aux\" in k): \n",
    "                    if abs(val) > 1e-3:\n",
    "                        metrics_log_aux += ' - %s: %.4f' % (k, val)\n",
    "                        continue      \n",
    "                    else:\n",
    "                        metrics_log_aux += ' - %s: %.4e' % (k, val)\n",
    "                        continue      \n",
    "                        \n",
    "                if abs(val) > 1e-3:\n",
    "                        metrics_log_default += ' - %s: %.4f' % (k, val)\n",
    "                else:\n",
    "                        metrics_log_default += ' - %s: %.4e' % (k, val)                        \n",
    "                    \n",
    "            print('step: {}/{} ... {}'.format(self.step,\n",
    "                                          self.params['epochs'],\n",
    "                                          metrics_log_default + \"\\n\" + metrics_log_main + \"\\n\" + metrics_log_aux))\n",
    "            self.metric_cache.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_embedding_RNN_layer(input_embedding):    \n",
    "    emb_x = GRU(128,activation=\"relu\",use_bias=False,recurrent_regularizer=regularizers.l1(0.0005),kernel_regularizer=regularizers.l1(0.005))(input_embedding)  \n",
    "    emb_x = BatchNormalization()(emb_x)    \n",
    "    return emb_x\n",
    "\n",
    "def level_siames_merge_RNN_layer(df_meta_input,name,bs):    \n",
    "    \n",
    "    input_embedding_1 = Input(shape=(None,1), name='embedding_input_1')        \n",
    "    input_embedding_2 = Input(shape=(None,1), name='embedding_input_2')        \n",
    "    \n",
    "    emb_x_1 = get_tensor_embedding_RNN_layer(input_embedding_1)    \n",
    "    emb_x_2 = get_tensor_embedding_RNN_layer(input_embedding_2)    \n",
    "    \n",
    "    L1_layer = Lambda(lambda tensors:(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([emb_x_1, emb_x_2])\n",
    "   \n",
    "    \n",
    "    emb_x_out = Dense(1, activation='sigmoid',name=\"aux\")(L1_distance)                \n",
    "    \n",
    "    meta_input = Input(shape=(len(df_meta_input.columns),), name='meta_input')    \n",
    "    \n",
    "    x = keras.layers.concatenate([L1_distance,meta_input])\n",
    "    x = Dense(256, activation='relu',use_bias=False,kernel_regularizer=regularizers.l1(0.0005))(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu',use_bias=False,kernel_regularizer=regularizers.l1(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu',use_bias=False,kernel_regularizer=regularizers.l1(0.0005))(x)   \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation='relu',use_bias=False,kernel_regularizer=regularizers.l1(0.0005))(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu',kernel_regularizer=regularizers.l1(0.0005))(x)    \n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation='relu',use_bias=False,kernel_regularizer=regularizers.l1(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    predictions = Dense(1, activation='sigmoid',name=\"main\")(x)\n",
    "        \n",
    "    model_created = Model(inputs=[input_embedding_1,input_embedding_2,meta_input], outputs=[predictions,emb_x_out])\n",
    "    model_created.Name = name\n",
    "    return model_created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor_embedding_layer(input_embedding):    \n",
    "    emb_x = Dense(512, activation='elu',use_bias=False,kernel_regularizer=regularizers.l2(0.001))(input_embedding)       \n",
    "    emb_x = BatchNormalization()(emb_x)\n",
    "    emb_x = Dropout(0.4)(emb_x)    \n",
    "    emb_x = Dense(256,use_bias = False, activation='elu',kernel_regularizer=regularizers.l1(0.0005))(emb_x)            \n",
    "    emb_x = BatchNormalization()(emb_x)    \n",
    "    emb_x = Dropout(0.4)(emb_x)    \n",
    "    emb_x = Dense(128,use_bias = False, activation='elu',kernel_regularizer=regularizers.l1(0.0005))(emb_x)            \n",
    "    emb_x = BatchNormalization()(emb_x)    \n",
    "    emb_x = Dense(64, use_bias = False, activation='elu',kernel_regularizer=regularizers.l1(0.0005))(emb_x)            \n",
    "    emb_x = BatchNormalization()(emb_x)    \n",
    "    emb_x = Dense(32, use_bias = False, activation='elu',kernel_regularizer=regularizers.l1(0.0005))(emb_x)                \n",
    "    emb_x = BatchNormalization()(emb_x)    \n",
    "    return emb_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_siames_merge_layer(df_meta_input,df_embeddings_1,df_embeddings_2,name,bs):    \n",
    "    \n",
    "    input_embedding_1 = Input(shape=(len(df_embeddings_1.columns),), name='embedding_input_1')        \n",
    "    input_embedding_2 = Input(shape=(len(df_embeddings_2.columns),), name='embedding_input_2')        \n",
    "    \n",
    "    emb_x_1 = get_tensor_embedding_layer(input_embedding_1)    \n",
    "    emb_x_2 = get_tensor_embedding_layer(input_embedding_2)    \n",
    "    \n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([emb_x_1, emb_x_2])\n",
    "    \n",
    "    emb_x_out = Dense(1, activation='sigmoid',name=\"aux\")(L1_distance)                \n",
    "    \n",
    "    meta_input = Input(shape=(len(df_meta_input.columns),), name='meta_input')    \n",
    "    \n",
    "    x = keras.layers.concatenate([L1_distance,meta_input])\n",
    "    x = Dense(128, activation='relu',use_bias=False)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.001))(x)   \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.001))(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.0005))(x)    \n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(8, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    predictions = Dense(1, activation='sigmoid',name=\"main\")(x)\n",
    "        \n",
    "    model_created = Model(inputs=[input_embedding_1,input_embedding_2,meta_input], outputs=[predictions,emb_x_out])\n",
    "    model_created.Name = name\n",
    "    return model_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_siames_merge_layer_soft(df_meta_input,df_embeddings_1,df_embeddings_2,name,bs):    \n",
    "    \n",
    "    input_embedding_1 = Input(shape=(len(df_embeddings_1.columns),), name='embedding_input_1')        \n",
    "    input_embedding_2 = Input(shape=(len(df_embeddings_2.columns),), name='embedding_input_2')        \n",
    "    \n",
    "    emb_x_1 = get_tensor_embedding_layer(input_embedding_1)    \n",
    "    emb_x_2 = get_tensor_embedding_layer(input_embedding_2)    \n",
    "    \n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([emb_x_1, emb_x_2])\n",
    "    \n",
    "    emb_x_out = Dense(3, activation='softmax',name=\"aux\")(L1_distance)                \n",
    "    \n",
    "    meta_input = Input(shape=(len(df_meta_input.columns),), name='meta_input')    \n",
    "    \n",
    "    x = keras.layers.concatenate([L1_distance,meta_input])\n",
    "    x = Dense(128, activation='relu',use_bias=False)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.001))(x)   \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.001))(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.0005))(x)    \n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(8, activation='relu',use_bias=False,kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    predictions = Dense(3, activation='softmax',name=\"main\")(x)\n",
    "        \n",
    "    model_created = Model(inputs=[input_embedding_1,input_embedding_2,meta_input], outputs=[predictions,emb_x_out])\n",
    "    model_created.Name = name\n",
    "    return model_created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x,y):    \n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_siames(train_x,train_emb_x_1,train_emb_x_2,train_y,val_x,val_emb_x_1,val_emb_x_2,val_y,model_train,n_epochs,optimizer,batchsize,loss_weigths,verb):\n",
    "    tensorboard = TensorBoard(log_dir= train_filepath + working_level + \"/board_logs/\" + model_train.Name + \"-{}\".format(time()))\n",
    "    checkpoint = ModelCheckpoint(train_filepath + working_level + \"/model_checkpoints/{0}-check-{{epoch:02d}}-{{val_main_acc:.2f}}.hdf5\".format(model_train.Name),save_weights_only=True,  period = int(n_epochs/5))\n",
    "    best_model_save = ModelCheckpoint(train_filepath + working_level + \"/model_checkpoints/{0}-best.hdf5\".format(model_train.Name), monitor='val_main_acc', save_weights_only=True, save_best_only=True, mode='max')\n",
    "    logger = EpochLogger(display=25)\n",
    "\n",
    "    model_train.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy',km.binary_recall()],loss_weights=loss_weigths)    \n",
    "    return model_train.fit([train_emb_x_1,train_emb_x_2,train_x], y = [train_y,train_y],verbose = verb,validation_data=([val_emb_x_1,val_emb_x_2,val_x],[val_y,val_y]),epochs = n_epochs,batch_size=batchsize,callbacks = [tensorboard,checkpoint,best_model_save,logger])  # starts training\n",
    "\n",
    "\n",
    "def fit_model_siames_soft(train_x,train_emb_x_1,train_emb_x_2,train_y,val_x,val_emb_x_1,val_emb_x_2,val_y,model_train,n_epochs,optimizer,batchsize,loss_weigths,verb):\n",
    "    tensorboard = TensorBoard(log_dir=train_filepath + working_level + \"/board_logs/\" + model_train.Name + \"-{}\".format(time()))\n",
    "    checkpoint = ModelCheckpoint(train_filepath + working_level + \"/model_checkpoints/{0}-check-{{epoch:02d}}-{{val_main_acc:.2f}}.hdf5\".format(model_train.Name),save_weights_only=True,  period = int(n_epochs/5))\n",
    "    best_model_save = ModelCheckpoint(train_filepath + working_level + \"/model_checkpoints/{0}-best.hdf5\".format(model_train.Name), monitor='val_main_acc', save_weights_only=True, save_best_only=True, mode='max')\n",
    "    logger = EpochLogger(display=25)\n",
    "\n",
    "    model_train.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy',km.sparse_categorical_recall()],loss_weights=loss_weigths)    \n",
    "    return model_train.fit([train_emb_x_1,train_emb_x_2,train_x], y = [train_y,train_y],verbose = verb,validation_data=([val_emb_x_1,val_emb_x_2,val_x],[val_y,val_y]),epochs = n_epochs,batch_size=batchsize,callbacks = [tensorboard,checkpoint,best_model_save,logger])  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(x_train_meta,x_train_1,x_train_2,y_train):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if(i == len(x_train_meta)):\n",
    "            i = 0\n",
    "        x_train_out = x_train_meta[i].reshape(1,len(x_train_meta[i]))\n",
    "                \n",
    "        x_train_1_out = x_train_1[i].reshape(1,len(x_train_1[i]),1)\n",
    "        x_train_2_out = x_train_2[i].reshape(1,len(x_train_2[i]),1)\n",
    "        y_train_out = y_train[i].reshape(1,1)\n",
    "        i += 1\n",
    "        yield [x_train_1_out,x_train_2_out,x_train_out], [y_train_out,y_train_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_generator(x_val_meta,x_val_1,x_val_2,y_val):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if(i == len(x_val_meta)):\n",
    "            i = 0\n",
    "        x_val_out = x_val_meta[i].reshape(1,len(x_val_meta[i]))\n",
    "                \n",
    "        x_val_1_out = x_val_1[i].reshape(1,len(x_val_1[i]),1)\n",
    "        x_val_2_out = x_val_2[i].reshape(1,len(x_val_2[i]),1)\n",
    "        y_val_out = y_val[i].reshape(1,1)\n",
    "        i += 1\n",
    "        yield [x_val_1_out,x_val_2_out,x_val_out], [y_val_out,y_val_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_siames_v2(train_x,train_emb_x_1,train_emb_x_2,train_y,val_x,val_emb_x_1,val_emb_x_2,val_y,model_train,n_epochs,optimizer,batchsize,loss_weigths,verb):\n",
    "    tensorboard = TensorBoard(log_dir= train_filepath + working_level + \"/board_logs/\" + model_train.Name + \"-{}\".format(time()))\n",
    "    checkpoint = ModelCheckpoint(train_filepath + working_level + \"/model_checkpoints/{0}-check-{{epoch:02d}}-{{val_main_acc:.2f}}.hdf5\".format(model_train.Name),save_weights_only=True,  period = int(n_epochs/5))\n",
    "    best_model_save = ModelCheckpoint(train_filepath + working_level + \"/model_checkpoints/{0}-best.hdf5\".format(model_train.Name), monitor='val_main_acc', save_weights_only=True, save_best_only=True, mode='max')\n",
    "    logger = EpochLogger(display=25)\n",
    "\n",
    "    model_train.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy',km.binary_recall()],loss_weights=loss_weigths)    \n",
    "    return model_train.fit_generator(train_generator(train_x,train_emb_x_1,train_emb_x_2,train_y),validation_data=val_generator(val_x,val_emb_x_1,val_emb_x_2,val_y),verbose = verb,epochs = n_epochs,steps_per_epoch=len(train_x)/batchsize,validation_steps=len(val_x)/batchsize ,callbacks = [tensorboard,checkpoint,best_model_save,logger])  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 608s - loss: 1.9905 - main_loss: 0.6964 - aux_loss: 0.7015 - main_acc: 0.5083 - main_recall: 0.8780 - aux_acc: 0.5250 - aux_recall: 0.8699 - val_loss: 0.9426 - val_main_loss: 0.6925 - val_aux_loss: 0.7246 - val_main_acc: 0.5333 - val_main_recall: 0.5161 - val_aux_acc: 0.4667 - val_aux_recall: 0.4839\n",
      "Epoch 2/50\n",
      " - 605s - loss: 0.9296 - main_loss: 0.6944 - aux_loss: 0.6984 - main_acc: 0.4917 - main_recall: 0.3162 - aux_acc: 0.4417 - aux_recall: 0.3162 - val_loss: 0.9288 - val_main_loss: 0.6941 - val_aux_loss: 0.7014 - val_main_acc: 0.4667 - val_main_recall: 0.0000e+00 - val_aux_acc: 0.4667 - val_aux_recall: 0.0000e+00\n",
      "Epoch 3/50\n",
      " - 579s - loss: 0.9257 - main_loss: 0.6929 - aux_loss: 0.6949 - main_acc: 0.5417 - main_recall: 0.0000e+00 - aux_acc: 0.5417 - aux_recall: 0.0000e+00 - val_loss: 0.9301 - val_main_loss: 0.6964 - val_aux_loss: 0.6982 - val_main_acc: 0.4667 - val_main_recall: 0.0000e+00 - val_aux_acc: 0.4667 - val_aux_recall: 0.0000e+00\n",
      "Epoch 4/50\n",
      " - 600s - loss: 0.9266 - main_loss: 0.6930 - aux_loss: 0.6973 - main_acc: 0.5417 - main_recall: 0.0000e+00 - aux_acc: 0.5417 - aux_recall: 0.0000e+00 - val_loss: 0.9165 - val_main_loss: 0.6864 - val_aux_loss: 0.6862 - val_main_acc: 0.5667 - val_main_recall: 0.0000e+00 - val_aux_acc: 0.5667 - val_aux_recall: 0.0000e+00\n",
      "Epoch 5/50\n"
     ]
    }
   ],
   "source": [
    "species_siames_dict = {}\n",
    "for specie in species_metadata:\n",
    "    dataset_complete_no_species = dataset_complete[dataset_complete[specie] == 0]    \n",
    "    dataset_complete_species = dataset_complete[dataset_complete[specie] == 1]    \n",
    "    \n",
    "    df_train,df_val,df_test = get_train_val_test(dataset_complete_no_species)\n",
    "    \n",
    "    df_train_x = df_train.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\",\"Sequence 1 Numerical\",\"Sequence 2 Numerical\"],axis=1)    \n",
    "\n",
    "    df_train_emb_x_1, df_train_emb_x_2 = get_numerical_embeddings(df_train)    \n",
    "    df_train_y = df_train[\"Is_Ohnolog\"]\n",
    "\n",
    "    df_val_x = df_val.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\",\"Sequence 1 Numerical\",\"Sequence 2 Numerical\"],axis=1)    \n",
    "\n",
    "    df_val_emb_x_1,df_val_emb_x_2 = get_numerical_embeddings(df_val)    \n",
    "    df_val_y = df_val[\"Is_Ohnolog\"]\n",
    "\n",
    "    df_test_x = df_test.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\",\"Sequence 1 Numerical\",\"Sequence 2 Numerical\"],axis=1)    \n",
    "\n",
    "    df_test_emb_x_1,df_test_emb_x_2 =  get_numerical_embeddings(df_test)    \n",
    "    df_test_y = df_test[\"Is_Ohnolog\"]\n",
    "    \n",
    "    \n",
    "    model = level_siames_merge_RNN_layer(df_train_x,\"Level_Siames_RNN\" + specie,128)\n",
    "    \n",
    "    log = fit_model_siames_v2(df_train_x.values,df_train_emb_x_1,df_train_emb_x_2,df_train_y.values,df_val_x.values,df_val_emb_x_1,df_val_emb_x_2,df_val_y.values,model,50,Adamax(),128,[1,0.3],2)\n",
    "    \n",
    "    break\n",
    "    df_test_species_x = dataset_complete_species.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\",\"Sequence 1 Numerical\",\"Sequence 2 Numerical\"],axis=1)    \n",
    "\n",
    "    df_test_species_emb_x_1 = dataset_complete_species[embedding_1_cols]\n",
    "    df_test_species_emb_x_2 = dataset_complete_species[embedding_2_cols]\n",
    "    df_test_species_y = dataset_complete_species[\"Is_Ohnolog\"]\n",
    "    \n",
    "    species_siames_dict[specie] = {}\n",
    "    \n",
    "    print(\"Evaluando especie Test: \" + specie)\n",
    "    metrics_names,metrics = (model.metrics_names,model.evaluate([df_test_emb_x_1.values,df_test_emb_x_2.values,df_test_x.values],[df_test_y.values,df_test_y.values], batch_size=256,verbose=False))\n",
    "    count = 0\n",
    "    for i in range(0,len(metrics)):\n",
    "        print(f'{metrics_names[i]}     = {metrics[i]:.4f}')  \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    species_siames_dict[specie][\"Test\"] = metrics[5]\n",
    "            \n",
    "        \n",
    "    print(\"Evaluando especie sin balancear: \" + specie)\n",
    "    metrics_names,metrics = (model.metrics_names,model.evaluate([df_test_species_emb_x_1.values,df_test_species_emb_x_2.values,df_test_species_x.values],[df_test_species_y.values,df_test_species_y.values], batch_size=256,verbose=False))\n",
    "    count = 0\n",
    "    for i in range(0,len(metrics)):\n",
    "        print(f'{metrics_names[i]}     = {metrics[i]:.4f}')  \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    species_siames_dict[specie][\"Sin Balanceo\"] = metrics[5]\n",
    "    \n",
    "    df_species_ohnologs = dataset_complete_species[dataset_complete_species[\"Is_Ohnolog\"] == 1]\n",
    "    df_species_no_ohnologs = dataset_complete_species[dataset_complete_species[\"Is_Ohnolog\"] == 0]\n",
    "    \n",
    "    if(len(df_species_ohnologs)>len(df_species_no_ohnologs)):\n",
    "        df_species_new = df_species_no_ohnologs.append(df_species_ohnologs.sample(len(df_species_no_ohnologs)))        \n",
    "    else:\n",
    "        df_species_new = df_species_ohnologs.append(df_species_no_ohnologs.sample(len(df_species_ohnologs)))\n",
    "\n",
    "    if(len(df_species_new) == 0):\n",
    "        df_species_new = df_species_ohnologs        \n",
    "                \n",
    "    df_test_species_x = df_species_new.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\",\"Sequence 1 Numerical\",\"Sequence 2 Numerical\"],axis=1)    \n",
    "\n",
    "    df_test_species_emb_x_1 =  df_species_new[embedding_1_cols]\n",
    "    df_test_species_emb_x_2 =  df_species_new[embedding_2_cols]\n",
    "    df_test_species_y = df_species_new[\"Is_Ohnolog\"]    \n",
    "    \n",
    "    print(\"Evaluando especie balanceada: \" + specie)\n",
    "    metrics_names,metrics = (model.metrics_names,model.evaluate([df_test_species_emb_x_1.values,df_test_species_emb_x_2.values,df_test_species_x.values],[df_test_species_y.values,df_test_species_y.values], batch_size=256,verbose=False))\n",
    "    count = 0\n",
    "    for i in range(0,len(metrics)):\n",
    "        print(f'{metrics_names[i]}     = {metrics[i]:.4f}')  \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    species_siames_dict[specie][\"Balanceadas\"] = metrics[5]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_siames_cross_dict = {}\n",
    "for specie in species_metadata:\n",
    "    dataset_complete_no_species = dataset_complete[dataset_complete[specie] == 0]    \n",
    "    dataset_complete_species = dataset_complete[dataset_complete[specie] == 1]    \n",
    "        \n",
    "    df_no_species = dataset_complete_no_species[dataset_complete_no_species[specie] == 0]\n",
    "    \n",
    "    df_train, df_val = train_test_split(df_no_species, test_size=0.2,random_state=9,stratify=df_no_species[\"Is_Ohnolog\"])\n",
    "    \n",
    "    df_train_x = df_train.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\"],axis=1)\n",
    "    df_train_x = df_train_x.drop(embedding_1_cols,axis=1)\n",
    "    df_train_x = df_train_x.drop(embedding_2_cols,axis=1)\n",
    "    \n",
    "    df_train_emb_x_1 = df_train[embedding_1_cols]\n",
    "    df_train_emb_x_2 = df_train[embedding_2_cols]\n",
    "\n",
    "    model = level_siames_merge_layer(df_train_x,df_train_emb_x_1,df_train_emb_x_2,\"Siames \" + specie,128)        \n",
    "    specie_metadata_copy = species_metadata[:]\n",
    "    species_metadata_no_species = specie_metadata_copy.remove(specie)\n",
    "\n",
    "    for specie_cross in species_metadata_no_species:\n",
    "        df_no_species = dataset_complete_no_species[dataset_complete_no_species[specie_cross] == 0]\n",
    "        df_train, df_val = train_test_split(df_no_species, test_size=0.2,random_state=9,stratify=df_no_species[\"Is_Ohnolog\"])\n",
    "\n",
    "        df_train_x = df_train.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\"],axis=1)\n",
    "        df_train_x = df_train_x.drop(embedding_1_cols,axis=1)\n",
    "        df_train_x = df_train_x.drop(embedding_2_cols,axis=1)\n",
    "\n",
    "        df_train_emb_x_1 = df_train[embedding_1_cols]\n",
    "        df_train_emb_x_2 = df_train[embedding_2_cols]\n",
    "        df_train_y = df_train[\"Is_Ohnolog\"]\n",
    "\n",
    "        df_val_x = df_val.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\"],axis=1)\n",
    "        df_val_x = df_val_x.drop(embedding_1_cols,axis=1)\n",
    "        df_val_x = df_val_x.drop(embedding_2_cols,axis=1)\n",
    "\n",
    "        df_val_emb_x_1 = df_val[embedding_1_cols]\n",
    "        df_val_emb_x_2 = df_val[embedding_2_cols]\n",
    "        df_val_y = df_val[\"Is_Ohnolog\"]\n",
    "\n",
    "        print(\"Entrenando sin especie: \" + species)\n",
    "        log = fit_model_siames(df_train_x.values,df_train_emb_x_1.values,df_train_emb_x_2.values,df_train_y.values,df_val_x.values,df_val_emb_x_1.values,df_val_emb_x_2.values,df_val_y.values,model,200,Adamax(),256,[1,0.2],0)\n",
    "        \n",
    "    \n",
    "    df_test_species_x = dataset_complete_species.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\"],axis=1)\n",
    "    df_test_species_x = df_test_species_x.drop(embedding_1_cols,axis=1)\n",
    "    df_test_species_x = df_test_species_x.drop(embedding_2_cols,axis=1)\n",
    "\n",
    "    df_test_species_emb_x_1 =  dataset_complete_species[embedding_1_cols]\n",
    "    df_test_species_emb_x_2 =  dataset_complete_species[embedding_2_cols]\n",
    "    df_test_species_y = dataset_complete_species[\"Is_Ohnolog\"]\n",
    "    \n",
    "    species_siames_cross_dict[specie] = {}\n",
    "    \n",
    "    print(\"Evaluando especie sin balancear: \" + specie)\n",
    "    metrics_names,metrics = (model.metrics_names,model.evaluate([df_test_species_emb_x_1.values,df_test_species_emb_x_2.values,df_test_species_x.values],[df_test_species_y.values,df_test_species_y.values], batch_size=256,verbose=False))\n",
    "    count = 0\n",
    "    for i in range(0,len(metrics)):\n",
    "        print(f'{metrics_names[i]}     = {metrics[i]:.4f}')  \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    species_siames_cross_dict[specie][\"Sin Balanceo\"] = metrics[5]\n",
    "    \n",
    "    df_species_ohnologs = dataset_complete_species[dataset_complete_species[\"Is_Ohnolog\"] == 1]\n",
    "    df_species_no_ohnologs = dataset_complete_species[dataset_complete_species[\"Is_Ohnolog\"] == 0]\n",
    "    \n",
    "    if(len(df_species_ohnologs)>len(df_species_no_ohnologs)):\n",
    "        df_species_new = df_species_no_ohnologs.append(df_species_ohnologs.sample(len(df_species_no_ohnologs)))        \n",
    "    else:\n",
    "        df_species_new = df_species_ohnologs.append(df_species_no_ohnologs.sample(len(df_species_ohnologs)))\n",
    "\n",
    "    if(len(df_species_new) == 0):\n",
    "        df_species_new = df_species_ohnologs        \n",
    "        \n",
    "    df_test_species_x = df_species_new.drop([\"Is_Ohnolog\",\"Is_Paralog\",\"Dup_Class\"],axis=1)\n",
    "    df_test_species_x = df_test_species_x.drop(embedding_1_cols,axis=1)\n",
    "    df_test_species_x = df_test_species_x.drop(embedding_2_cols,axis=1)\n",
    "\n",
    "    df_test_species_emb_x_1 = df_species_new[embedding_1_cols]\n",
    "    df_test_species_emb_x_2 = df_species_new[embedding_2_cols]\n",
    "    df_test_species_y = df_species_new[\"Is_Ohnolog\"]                    \n",
    "    \n",
    "    print(\"Evaluando especie balanceada: \" + specie)\n",
    "    metrics_names,metrics = (model.metrics_names,model.evaluate([df_test_species_emb_x_1.values,df_test_species_emb_x_2.values,df_test_species_x.values],[df_test_species_y.values,df_test_species_y.values], batch_size=256,verbose=False))\n",
    "    count = 0\n",
    "    for i in range(0,len(metrics)):\n",
    "        print(f'{metrics_names[i]}     = {metrics[i]:.4f}')  \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    species_cross_dict[specie][\"Balanceadas\"] = metrics[5]   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis3",
   "language": "python",
   "name": "thesis3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
